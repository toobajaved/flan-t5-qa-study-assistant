{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt9VAq029D70"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Move the FLAN-T5 Model and its tokenizer to the GPU\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Check if the tokenizer works by testing it on a sample text\n",
        "sample_text = \"I am testing\"\n",
        "tokens = tokenizer(sample_text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "rW3NLPik9IUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the original dataset and reduce its size\n",
        "ds = load_dataset(\"rajpurkar/squad\")\n",
        "train_ds = ds['train'].shuffle(seed=42).select(range(10000))  # Adjust the range if needed\n"
      ],
      "metadata": {
        "id": "Zb50S40s9Jod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the preprocess function\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the context and question\n",
        "    inputs = tokenizer(\n",
        "        examples['question'],\n",
        "        examples['context'],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Initialize a list to hold the first answers\n",
        "    first_answers = []\n",
        "    for answer in examples['answers']:\n",
        "        if len(answer['text']) > 0:  # Check if the text list is not empty\n",
        "            first_answers.append(answer['text'][0])  # Append the first answer\n",
        "        else:\n",
        "            first_answers.append(\"\")  # Append an empty string if no answers\n",
        "\n",
        "    # Tokenize the answers\n",
        "    labels = tokenizer(\n",
        "        first_answers,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )['input_ids']\n",
        "\n",
        "    inputs['labels'] = labels\n",
        "    return inputs\n",
        "\n",
        "# Preprocess the dataset using the reduced training set\n",
        "tokenized_ds = train_ds.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "NkkZYx6B9PAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the tokenized dataset into training and validation sets (80% train, 20% validation)\n",
        "train_test_split = tokenized_ds.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "val_dataset = train_test_split['test']\n"
      ],
      "metadata": {
        "id": "fs0YnD3C9RMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,  # Load the best checkpoint after training\n",
        ")\n"
      ],
      "metadata": {
        "id": "10MRKgt99Wda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Initialize the trainer with model, dataset, and training arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "Dos5wJoD9YNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate an answer based on a context and question\n",
        "def generate_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    outputs = model.generate(inputs.input_ids)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Example context and question\n",
        "context = \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was constructed between 1887 and 1889 as the entrance arch for the 1889 World's Fair.\"\n",
        "question = \"When was the Eiffel Tower constructed?\"\n",
        "\n",
        "# Generate the answer\n",
        "answer = generate_answer(context, question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "OykHxZTzSFtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "id": "t1-XASBySJ0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model and tokenizer\n",
        "model.save_pretrained(\"./trained_model\")\n",
        "tokenizer.save_pretrained(\"./trained_model\")\n"
      ],
      "metadata": {
        "id": "Fm9zci_MSyTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "id": "wZ3ps6aaS2M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the model and tokenizer to Hugging Face with a unique name\n",
        "model.push_to_hub(\"tootooba/flan-t5-qa-study-assistant\")\n",
        "tokenizer.push_to_hub(\"tootooba/flan-t5-qa-study-assistant\")\n"
      ],
      "metadata": {
        "id": "46wVgKBXUG1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}